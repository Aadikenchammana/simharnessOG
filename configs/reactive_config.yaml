ENV: Reactive-v1
SIMULATION: Fire-v2
ALGORITHM: DQN_GuidedEx
SAVE_PATH: ./outputs
SAVE_ID: &save_id DQN_GE_test
SAVE_MODEL_NAME: model
FEATURE_EXTRACTOR: FLATTENED_FE
NETWORK: MLP_EXTRACTOR
NUM_PROC: 16

RLHARNESS:
  # NONE movement and interaction is added by default at position 0 for both
  movements: &action_list ['up', 'down', 'left', 'right']
  interactions: ['fireline']
  attributes: &attribute_list ['fire_map', 'elevation', 'w_0', 'sigma', 'delta', 'M_x']
  normalized_attributes: ['elevation']
  agent_speed: 9
# --------------------------------------------

ALGO_COMMON:
  policy: DQNGuidedExPolicy
  tensorboard_log_dir: tensorboard
  verbose: 1
  seed: 4078
  learning_rate: 0.005

# --------------------------------------------

LEARN:
  total_timesteps: 10000000                 # Total number of time steps to train to (does not take episodes into account)
  log_interval: 10000                          # Number of iterations before logging information. If end of episode, set to known timestep (screensize * screensize)
  tb_log_name: *save_id                 # Name of tensorboard folder for this run

# --------------------------------------------

PPO:
  n_steps: 128                             # Number of steps in an iteration (number of steps before an update)
  batch_size: 64
  n_epochs: 10
  gamma: 1
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: False
  sde_sample_freq: -1

MaskablePPO:
  n_steps: 2048                             # Number of steps in an iteration (number of steps before an update)
  batch_size: 64
  n_epochs: 10
  gamma: 1
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5

A2C:
  n_steps: 1                             # Number of steps in an iteration (number of steps before an update)
  gamma: 1
  gae_lambda: 0.95
  ent_coef: 0.0
  vf_coef: 0.0
  max_grad_norm: 0.5
  rms_prop_eps: 0.00001
  use_rms_prop: True
  use_sde: False
  sde_sample_freq: -1
  normalize_advantage: False

DQN:
  buffer_size: 10000
  learning_starts: 1
  batch_size: 32
  tau: 1
  gamma: 1
  train_freq: 2
  gradient_steps: 1
  optimize_memory_usage: False
  target_update_interval: 1000
  exploration_fraction: 0.75
  exploration_initial_eps: 1.0
  exploration_final_eps: 0.05
  max_grad_norm: 10

DQN_ICM:
  buffer_size: 10000
  learning_starts: 30000
  batch_size: 32
  tau: 1
  gamma: 1
  beta: 0.2
  eta: 0.2
  train_freq: 2
  gradient_steps: 1
  optimize_memory_usage: False
  target_update_interval: 30000
  exploration_fraction: 0.75
  exploration_initial_eps: 1.0
  exploration_final_eps: 0.05
  max_grad_norm: 10

DQN_GuidedEx:
  buffer_size: 1000000
  learning_starts: 10000
  batch_size: 32
  tau: 1
  gamma: 0.99
  beta: 0.2
  eta: 0.2
  train_freq: 1000
  gradient_steps: 1
  optimize_memory_usage: False
  target_update_interval: 1000
  exploration_fraction: 0.9
  exploration_initial_eps: 1.0
  exploration_final_eps: 0.01
  max_grad_norm: 10
  initial_random_steps: 100000
# --------------------------------------------

CALLBACKS:
  USE_STOP_MAX_EPISODES: False
  USE_EVAL: True
  USE_CHECKPOINT: True
  VERBOSE: 1

  CHECKPOINT:                             # Save the current model every X timesteps
    save_freq: 25000000
    save_dir: checkpoints
    name_prefix: ckpt

  STOP_MAX_EPISODES:                      # Stop training after X episodes
    max_episodes: 200

  EVAL:                                   # Evaluate the model every X timesteps
    eval_freq: 5000000
    n_eval_episodes: 10
    log_dir: eval_logs
    best_model_save_dir: eval_models
    deterministic: True
    render: False

# --------------------------------------------

POLICY_KWARGS:

  COMMON:
    normalize_images: False
    activation_fn: ReLU

  # --------------------------------------------

  GRID_STATE_FE:
    # With a 2x64x64 input, should be 64x16x16 output
    # Output_Channel = [(Input_Channel - Kernel + 2(Padding)) / Stride] + 1
    out_channels: [32, 32, 64]
    kernels: [8, 4, 1]
    strides: [4, 2, 1]
    padding: [0, 0, 0]
    features_dim: 14336
    flatten: True

  UNET_FE:
    depth: &unet_down_depth 4
    attributes: *attribute_list
    init_features: &unet_down_init_features 32

  FLATTENED_FE:

  # --------------------------------------------

  GRIDWISE_ACTION_NET_ARCH:
    # Output_Channel = (Input_Channel − 1) * S − 2P + dilation * (K−1) + OP + 1
    # Reverse the input kernels + padding to go back to original spatial size
    pi_channels: [32, 8]
    pi_kernels: [2, 4]
    pi_strides: [2, 2]
    pi_padding: [0, 1]
    pi_output_padding: [0, 0]
    pi_dilation: [1, 1]
    pi_fc: []
    vf_dims: [2056, 1028, 64]

  MLP_EXTRACTOR:
    #net_arch: [128, {'vf': [64], 'pi': [64]}]
    net_arch: [4096, 100]

  UNET_ACTION_NET_ARCH:
    policy_depth: *unet_down_depth
    value_depth: 2
    policy_init_features: *unet_down_init_features
    actions: *action_list
    policy_activation_function: sigmoid
    value_activation_function: relu
